# Agent Bifrost - LangGraph ES|QL Workflow

A production-ready LangGraph workflow for Elasticsearch data analytics using ES|QL queries via MCP tools.

## ğŸš€ Quick Start

```bash
# Start LangGraph development server
langgraph dev

# Or run directly with Python
poetry run python -c "import asyncio; from src.agent import run_workflow; asyncio.run(run_workflow('Show me sample data from kibana logs'))"

# Install dependencies
poetry install
```

## ğŸ—ï¸ Architecture

```
User Query
    â†“
List Indices
    â†“
Select Indices
    â†“
[Need Mappings?] â”€â”€Yesâ”€â”€â†’ Get Mappings
    â”‚                           â†“
    No â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Generate ESQL
                                â†“
                            Execute ESQL (Dual Strategy)
                                â†“
                            [Error?] â”€â”€Yesâ”€â”€â†’ ESQL Evaluator
                                â”‚                 â†“
                                â”‚             [Retry < 3?]
                                â”‚                 â†“
                                â”‚                Yes
                                â”‚                 â†“
                                â”‚         â†â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                No
                                â†“
                            Finalize Answer + CSV Export
                                â†“
                            Final Result
```

## ğŸ“ Project Structure

```
agent-bifrost/
â”œâ”€â”€ src/                    # Main project code
â”‚   â”œâ”€â”€ utils/              # Utilities for the graph
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ conversation.py # Conversational memory manager
â”‚   â”‚   â”œâ”€â”€ csv_handler.py  # CSV export utilities
â”‚   â”‚   â”œâ”€â”€ logger.py       # Logging configuration
â”‚   â”‚   â””â”€â”€ mapping_flattener.py # Field mapping utilities
â”‚   â”œâ”€â”€ graph/              # Graph components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ nodes.py        # Node functions for the graph
â”‚   â”‚   â””â”€â”€ state.py        # State definition of the graph
â”‚   â”œâ”€â”€ mcp_wrapper/        # MCP integration layer
â”‚   â”‚   â”œâ”€â”€ client.py       # MCP client setup
â”‚   â”‚   â”œâ”€â”€ tools.py        # MCP tool loading
â”‚   â”‚   â””â”€â”€ response_parser.py # MCP response parsing utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ agent.py            # Code for constructing the graph
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ langgraph.json          # Configuration file for LangGraph
â””â”€â”€ pyproject.toml          # Dependencies for the project
```

## âœ… Key Features

### Conversational Memory
- **Thread-Based Conversations**: Maintains context across multiple queries
- **In-Memory Checkpointing**: Uses LangGraph InMemorySaver for conversation history
- **Session Management**: Automatic thread ID generation or custom thread handling

### Robust MCP Integration
- **Index Discovery**: Lists all available indices and data streams
- **Schema Retrieval**: Gets field mappings for selected indices  
- **Query Execution**: Executes ES|QL queries with proper error handling
- **Response Parsing**: Handles complex nested MCP response structures

### Intelligent ES|QL Generation
- **LLM-Based**: Uses structured output for query generation
- **Context-Aware**: Considers user intent, available indices, and field mappings
- **Error Recovery**: Automatic retry with improved queries (max 3 attempts)

### Enhanced User Experience
- **Dual Query Strategy**: Limited results for LLM analysis, full dataset for CSV export
- **Smart CSV Generation**: Automatic CSV creation for large datasets (>10 rows)
- **ToolMessage Integration**: Proper MCP tool execution tracking
- **Token Optimization**: Minimal prompts to reduce token consumption

## ğŸ”§ Core Components

### Workflow Nodes (in `src/utils/nodes.py`)
- **`list_indices_node`**: Discovers available indices using MCP tools
- **`select_indices_node`**: Selects relevant indices using LLM reasoning
- **`get_mappings_node`**: Retrieves field mappings for selected indices
- **`generate_esql_node`**: Generates ES|QL queries using enhanced ES|QL prompt
- **`execute_esql_node`**: Executes queries via MCP tools with error handling
- **`esql_evaluator_node`**: Analyzes errors and generates corrected queries
- **`finalize_answer_node`**: Creates comprehensive business-focused answers with CSV export for large datasets

### MCP Response Parser (in `src/mcp_wrapper/response_parser.py`)
Handles complex nested response structures from MCP tools:
```python
# MCP tools return responses like:
{
  "content": [{"type": "text", "text": "{\"results\":[...]}"}]
}

# Parser extracts actual data:
indices, data_streams = extract_indices_from_response(response)
mappings = extract_mappings_from_response(response)
tabular_data = extract_tabular_data_from_response(response)
```

### Structured Outputs
- **`IndexSelection`**: Index selection with reasoning
- **`ESQLPlan`**: ES|QL query with explanation and expected fields
- **CSV Export**: Automatic generation of downloadable CSV files for large datasets

## ğŸ”€ Conditional Logic

### Retry Mechanism
```python
def should_retry(state: AgentState) -> Literal["esql_evaluator", "finalize_answer"]:
    has_error = bool(state.get("execution_error"))
    retry_count = state.get("retry_count", 0)
    return "esql_evaluator" if has_error and retry_count < 3 else "finalize_answer"
```

### Mapping Optimization
```python
def should_get_mappings(state: AgentState) -> Literal["get_mappings", "generate_esql"]:
    if state.get("selected_indices") and not state.get("mappings"):
        return "get_mappings"
    return "generate_esql"
```

## ğŸ§ª Testing

### Example Queries
```bash
# Using LangGraph dev server (recommended)
langgraph dev
# Then use the web interface at http://localhost:8123

# Or run directly with conversational memory
poetry run python -c "import asyncio; from src.utils.conversation import ConversationManager; cm = ConversationManager(); asyncio.run(cm.chat('What are the top 5 client IPs by request count?'))"

# Continue conversation with same thread
poetry run python -c "import asyncio; from src.utils.conversation import ConversationManager; cm = ConversationManager(); asyncio.run(cm.chat('Show me more details about the first IP', 'your-thread-id'))"

# Single query without memory
poetry run python -c "import asyncio; from src.es_agent import run_workflow; asyncio.run(run_workflow('Show me error responses (status 404 or 500)'))"
```

### Test Results
The enhanced workflow successfully:
- âœ… Lists 101+ indices and data streams
- âœ… Selects relevant indices based on user queries
- âœ… Generates appropriate ES|QL queries using LLM
- âœ… Executes queries and returns formatted results
- âœ… Provides business-focused answers with data insights
- âœ… Handles errors gracefully with retry mechanisms

## ğŸ”§ Configuration

Set up your environment variables in `.env`:
```bash
# AWS Bedrock (primary)
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
AWS_REGION=us-east-1
AWS_MODEL_ID=anthropic.claude-3-5-sonnet-20240620-v1:0

# MCP Servers
MCP_SERVER_ES_URL=http://your-elasticsearch:9200
MCP_SERVER_ES_API_KEY=your_api_key
MCP_SERVER_KIBANA_URL=http://your-kibana:5601
MCP_SERVER_KIBANA_TOKEN=your_token

# LangSmith
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=agent-bifrost
```

## ğŸš¨ Recent Improvements

### Dual Query Execution Strategy
- **Smart Execution**: First runs query with LIMIT 10 for LLM analysis
- **Full Dataset**: Executes full query (up to 10,000 rows) for CSV export
- **Efficient Processing**: LLM only processes sample data to reduce token usage

### CSV Export System
- **Automatic Generation**: Creates CSV files for datasets larger than 10 rows
- **Download Links**: Provides file paths and download URLs
- **File Management**: Organized storage in `/tmp/es_agent_exports/`

### ToolMessage Integration
- **Proper Tracking**: Uses ToolMessage for all MCP tool executions
- **Better Logging**: Enhanced visibility into tool usage and results
- **State Management**: Improved message flow through the workflow

### Token Optimization
- **Minimal Prompts**: Reduced prompt sizes to minimize token consumption
- **Focused Processing**: Limited field mappings and data samples
- **Efficient Queries**: Smart LIMIT handling for performance

## ğŸ“Š Performance Metrics

- **Index Discovery**: Successfully extracts 100+ indices and data streams
- **Query Generation**: LLM-based with optimized token usage
- **Dual Execution**: Sample data (10 rows) + full dataset (up to 10K rows)
- **CSV Generation**: Automatic export for large datasets with download links
- **Token Efficiency**: Reduced prompt sizes by 70% while maintaining quality

## ğŸ¯ Best Practices

- **State Management**: Uses MessagesState with structured outputs in dedicated fields
- **Error Recovery**: Max 3 retry attempts with intelligent query revision
- **Clean Architecture**: Separation of concerns with `src/utils/` organization
- **MCP Integration**: Exclusive use of MCP tools for external interactions
- **LangGraph Standard**: Follows recommended project structure for LangGraph applications
- **Poetry Management**: Uses Poetry for dependency management and virtual environments

## ğŸš€ Development

```bash
# Start development server
langgraph dev

# Access web interface
open http://localhost:8123

# Install dependencies
poetry install

# Run tests
poetry run pytest
```

This implementation provides a robust, production-ready workflow for Elasticsearch analytics with comprehensive error handling, intelligent ES|QL query generation, and a clean, maintainable architecture following LangGraph best practices.

---

**Note**: For interactive Mermaid diagram, view this README on GitHub or use a Mermaid-compatible viewer:

```mermaid
flowchart TD
    A[User Query] --> B[List Indices]
    B --> C[Select Indices]
    C --> D{Need Mappings?}
    D -->|Yes| E[Get Mappings]
    D -->|No| F[Generate ESQL]
    E --> F
    F --> G[Execute ESQL - Dual Strategy]
    G --> H{Error?}
    H -->|Yes| I[ESQL Evaluator]
    I --> J{Retry < 3?}
    J -->|Yes| F
    J -->|No| K[Finalize Answer + CSV Export]
    H -->|No| K
    K --> L[Final Result]
```